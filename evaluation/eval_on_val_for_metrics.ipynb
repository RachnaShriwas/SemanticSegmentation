{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "root = \"/Users/rachna/Desktop/Python2_venv/\"\n",
    "sys.path.append(root + \"SemanticSegmentation\")\n",
    "from datasets import DatasetVal \n",
    "\n",
    "sys.path.append(root + \"SemanticSegmentation/model\")\n",
    "from model import Model\n",
    "\n",
    "sys.path.append(root +\"SemanticSegmentation/utils\")\n",
    "from utils import label_img_to_color\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainId_to_id = {\n",
    "    0: 7,\n",
    "    1: 8,\n",
    "    2: 11,\n",
    "    3: 12,\n",
    "    4: 13,\n",
    "    5: 17,\n",
    "    6: 19,\n",
    "    7: 20,\n",
    "    8: 21,\n",
    "    9: 22,\n",
    "    10: 23,\n",
    "    11: 24,\n",
    "    12: 25,\n",
    "    13: 26,\n",
    "    14: 27,\n",
    "    15: 28,\n",
    "    16: 31,\n",
    "    17: 32,\n",
    "    18: 33,\n",
    "    19: 0\n",
    "}\n",
    "trainId_to_id_map_func = np.vectorize(trainId_to_id.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained resnet, 18\n",
      "('num_val_batches:', 250)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "network = Model(\"eval_val_for_metrics\", project_dir=root+\"SemanticSegmentation\")\n",
    "network.load_state_dict(torch.load(root+\"SemanticSegmentation/pretrained_models/model_13_2_2_2_epoch_580.pth\", map_location='cpu'))\n",
    "\n",
    "val_dataset = DatasetVal(cityscapes_data_path=root+\"SemanticSegmentation/data_dir/cityscapes\",\n",
    "                         cityscapes_meta_path=root+\"SemanticSegmentation/data_dir/cityscapes/meta\")\n",
    "\n",
    "num_val_batches = int(len(val_dataset)/batch_size)\n",
    "print (\"num_val_batches:\", num_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=batch_size, shuffle=False,\n",
    "                                         num_workers=0)\n",
    "\n",
    "with open(root+\"SemanticSegmentation/data_dir/cityscapes/meta/class_weights.pkl\", \"rb\") as file: \n",
    "    class_weights = np.array(pickle.load(file))\n",
    "class_weights = torch.from_numpy(class_weights)\n",
    "class_weights = Variable(class_weights.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.504262\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "network.eval() \n",
    "batch_losses = []\n",
    "for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n",
    "    with torch.no_grad(): \n",
    "        imgs = Variable(imgs)# (shape: (batch_size, 3, img_h, img_w))\n",
    "        label_imgs = Variable(label_imgs.type(torch.LongTensor))#.cuda() # (shape: (batch_size, img_h, img_w))\n",
    "\n",
    "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
    "\n",
    "        # compute the loss:\n",
    "        loss = loss_fn(outputs, label_imgs)\n",
    "        loss_value = loss.data.cpu().numpy()\n",
    "        batch_losses.append(loss_value)\n",
    "\n",
    "        outputs = F.upsample(outputs, size=(1024, 2048), mode=\"bilinear\") # (shape: (batch_size, num_classes, 1024, 2048))\n",
    "\n",
    "        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, 1024, 2048))\n",
    "        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, 1024, 2048))\n",
    "        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
    "\n",
    "        for i in range(pred_label_imgs.shape[0]):\n",
    "            pred_label_img = pred_label_imgs[i] # (shape: (1024, 2048))\n",
    "            img_id = img_ids[i]\n",
    "\n",
    "            # convert pred_label_img from trainId to id pixel values:\n",
    "            pred_label_img = trainId_to_id_map_func(pred_label_img) # (shape: (1024, 2048))\n",
    "            pred_label_img = pred_label_img.astype(np.uint8)\n",
    "\n",
    "            cv2.imwrite(network.model_dir + \"/\" + img_id + \"_pred_label_img.png\", pred_label_img)\n",
    "\n",
    "val_loss = np.mean(batch_losses)\n",
    "print (\"Val loss: %g\" % val_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
