{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from datasets import DatasetVal \n",
    "\n",
    "sys.path.append(\"../model\")\n",
    "from model import Model\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from utils import label_img_to_color\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "path = \"Users/rachna/Desktop/Python2_venv/SemanticSegmentation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained resnet, 18\n",
      "('num_val_batches:', 250)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "network = Model(\"eval_val\", project_dir=path)\n",
    "network.load_state_dict(torch.load(path+\"pretrained_models/model_13_2_2_2_epoch_580.pth\", map_location=\"cpu\"))\n",
    "\n",
    "val_dataset = DatasetVal(cityscapes_data_path=path+\"data_dir/cityscapes\",\n",
    "                         cityscapes_meta_path=path+\"data_dir/cityscapes/meta\")\n",
    "\n",
    "num_val_batches = int(len(val_dataset)/batch_size)\n",
    "print (\"num_val_batches:\", num_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=batch_size, shuffle=False,\n",
    "                                         num_workers=0)\n",
    "\n",
    "with open(path+\"data_dir/cityscapes/meta/class_weights.pkl\", \"rb\") as file: \n",
    "    class_weights = np.array(pickle.load(file))\n",
    "class_weights = torch.from_numpy(class_weights)\n",
    "class_weights = Variable(class_weights.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.504262\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "network.eval()\n",
    "batch_losses = []\n",
    "for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n",
    "    with torch.no_grad(): \n",
    "        imgs = Variable(imgs) # (shape: (batch_size, 3, img_h, img_w))\n",
    "        label_imgs = Variable(label_imgs.type(torch.LongTensor))#.cuda() # (shape: (batch_size, img_h, img_w))\n",
    "\n",
    "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
    "\n",
    "        # compute the loss:\n",
    "        loss = loss_fn(outputs, label_imgs)\n",
    "        loss_value = loss.data.cpu().numpy()\n",
    "        batch_losses.append(loss_value)\n",
    "\n",
    "        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n",
    "        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n",
    "        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
    "\n",
    "        for i in range(pred_label_imgs.shape[0]):\n",
    "            if i == 0:\n",
    "                pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n",
    "                img_id = img_ids[i]\n",
    "                img = imgs[i] # (shape: (3, img_h, img_w))\n",
    "\n",
    "                img = img.data.cpu().numpy()\n",
    "                img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n",
    "                img = img*np.array([0.229, 0.224, 0.225])\n",
    "                img = img + np.array([0.485, 0.456, 0.406])\n",
    "                img = img*255.0\n",
    "                img = img.astype(np.uint8)\n",
    "\n",
    "                pred_label_img_color = label_img_to_color(pred_label_img)\n",
    "                overlayed_img = 0.35*img + 0.65*pred_label_img_color\n",
    "                overlayed_img = overlayed_img.astype(np.uint8)\n",
    "\n",
    "                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_overlayed.png\", overlayed_img)\n",
    "\n",
    "val_loss = np.mean(batch_losses)\n",
    "print (\"Val Loss: %g\" % val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
